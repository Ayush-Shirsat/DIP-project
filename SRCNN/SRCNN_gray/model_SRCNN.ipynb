{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import all libraries\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import SGD,RMSprop,Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Disable warning message of tensorflow\n",
    "import cv2\n",
    "from numpy import *\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage.util.shape import view_as_windows\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/util/shape.py:255: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  indexing_strides = arr_in[slices].strides\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396900\n"
     ]
    }
   ],
   "source": [
    "# Load X data\n",
    "path1 = './Input/'\n",
    "X_data = os.listdir(path1)\n",
    "X_data = sorted(X_data)\n",
    "\n",
    "X = []\n",
    "window_shape = (32, 32)\n",
    "for img in X_data:\n",
    "    im = cv2.imread(path1 + img,0)\n",
    "    patches = view_as_windows(im, window_shape, step=16)\n",
    "    patches = patches.reshape(patches.shape[0]*patches.shape[1],32,32)\n",
    "    for i in range(0,len(patches)):\n",
    "        X.append(patches[i])\n",
    "        \n",
    "X = np.array(X)\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/util/shape.py:255: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  indexing_strides = arr_in[slices].strides\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396900\n"
     ]
    }
   ],
   "source": [
    "# Load Y data\n",
    "path2 = './Label/'\n",
    "Y_data = os.listdir(path2)\n",
    "Y_data = sorted(Y_data)\n",
    "\n",
    "Y = []\n",
    "window_shape = (32, 32)\n",
    "for img2 in Y_data:\n",
    "    im2 = cv2.imread(path2 + img2,0)\n",
    "    patches = view_as_windows(im2, window_shape, step=16)\n",
    "    patches = patches.reshape(patches.shape[0]*patches.shape[1],32,32)\n",
    "    for i in range(0,len(patches)):\n",
    "        Y.append(patches[i])\n",
    "\n",
    "Y = np.array(Y)\n",
    "print(len(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317520, 32, 32, 1) (79380, 32, 32, 1) (317520, 32, 32, 1) (79380, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 4)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 1)\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], 32, 32, 1)\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], 32, 32, 1)\n",
    "\n",
    "X_train = X_train.astype('int') \n",
    "X_test = X_test.astype('int')\n",
    "Y_train = Y_train.astype('int') \n",
    "Y_test = Y_test.astype('int')\n",
    "\n",
    "# Normalization of data \n",
    "# Data pixels are between 0 and 1\n",
    "# X_train /= 255\n",
    "# X_test /= 255\n",
    "# Y_train /= 255\n",
    "# Y_test /= 255\n",
    "\n",
    "print(np.shape(X_train),np.shape(X_test),np.shape(Y_train),np.shape(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /share/pkg/python/3.6.2/install/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 128)       10496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        8256      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 1)         1601      \n",
      "=================================================================\n",
      "Total params: 20,353\n",
      "Trainable params: 20,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(128, (9, 9), activation='relu', padding='same', input_shape=(32, 32, 1)))\n",
    "model.add(Conv2D(64, (1, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(1, (5, 5), activation='linear', padding='same'))\n",
    "\n",
    "opt = Adam(lr = 0.003)\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer = opt) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 254016 samples, validate on 63504 samples\n",
      "Epoch 1/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 8.3327Epoch 00000: loss improved from inf to 8.32597, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 18s - loss: 8.3260 - val_loss: 5.2148\n",
      "Epoch 2/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 5.1269Epoch 00001: loss improved from 8.32597 to 5.12904, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 5.1290 - val_loss: 5.0445\n",
      "Epoch 3/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 5.4213Epoch 00002: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 5.4219 - val_loss: 5.0058\n",
      "Epoch 4/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.9567Epoch 00003: loss improved from 5.12904 to 4.95679, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.9568 - val_loss: 4.7368\n",
      "Epoch 5/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.8292Epoch 00004: loss improved from 4.95679 to 4.83003, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.8300 - val_loss: 4.7044\n",
      "Epoch 6/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.8562Epoch 00005: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.8565 - val_loss: 4.6901\n",
      "Epoch 7/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.7944Epoch 00006: loss improved from 4.83003 to 4.79459, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.7946 - val_loss: 4.9406\n",
      "Epoch 8/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.7934Epoch 00007: loss improved from 4.79459 to 4.79328, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.7933 - val_loss: 4.7278\n",
      "Epoch 9/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.7413Epoch 00008: loss improved from 4.79328 to 4.74124, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.7412 - val_loss: 4.7326\n",
      "Epoch 10/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.7760Epoch 00009: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.7764 - val_loss: 4.7953\n",
      "Epoch 11/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.7195Epoch 00010: loss improved from 4.74124 to 4.71943, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.7194 - val_loss: 4.6512\n",
      "Epoch 12/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 5.9054Epoch 00011: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 5.9518 - val_loss: 18.4585\n",
      "Epoch 13/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 5.9715Epoch 00012: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 5.9694 - val_loss: 5.0567\n",
      "Epoch 14/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.9209Epoch 00013: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.9204 - val_loss: 4.9652\n",
      "Epoch 15/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.8366Epoch 00014: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.8367 - val_loss: 4.7704\n",
      "Epoch 16/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.9046Epoch 00015: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.9053 - val_loss: 4.9074\n",
      "Epoch 17/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.8535Epoch 00016: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.8531 - val_loss: 4.7245\n",
      "Epoch 18/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.7750Epoch 00017: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.7756 - val_loss: 4.7646\n",
      "Epoch 19/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.8480Epoch 00018: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.8482 - val_loss: 4.7116\n",
      "Epoch 20/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.8074Epoch 00019: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.8067 - val_loss: 4.7786\n",
      "Epoch 21/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.7775Epoch 00020: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.7775 - val_loss: 5.0534\n",
      "Epoch 22/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.7350Epoch 00021: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.7350 - val_loss: 4.6564\n",
      "Epoch 23/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.6124Epoch 00022: loss improved from 4.71943 to 4.61191, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.6119 - val_loss: 4.6395\n",
      "Epoch 24/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.6029Epoch 00023: loss improved from 4.61191 to 4.60297, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.6030 - val_loss: 4.6340\n",
      "Epoch 25/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5972Epoch 00024: loss improved from 4.60297 to 4.59685, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5968 - val_loss: 4.6273\n",
      "Epoch 26/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5911Epoch 00025: loss improved from 4.59685 to 4.59102, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5910 - val_loss: 4.6240\n",
      "Epoch 27/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5849Epoch 00026: loss improved from 4.59102 to 4.58451, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5845 - val_loss: 4.6351\n",
      "Epoch 28/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5799Epoch 00027: loss improved from 4.58451 to 4.57956, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5796 - val_loss: 4.6080\n",
      "Epoch 29/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5745Epoch 00028: loss improved from 4.57956 to 4.57472, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5747 - val_loss: 4.6167\n",
      "Epoch 30/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5699Epoch 00029: loss improved from 4.57472 to 4.56965, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5696 - val_loss: 4.6125\n",
      "Epoch 31/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5670Epoch 00030: loss improved from 4.56965 to 4.56687, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5669 - val_loss: 4.5919\n",
      "Epoch 32/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5609Epoch 00031: loss improved from 4.56687 to 4.56128, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5613 - val_loss: 4.5864\n",
      "Epoch 33/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5561Epoch 00032: loss improved from 4.56128 to 4.55591, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.5559 - val_loss: 4.5852\n",
      "Epoch 34/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5534Epoch 00033: loss improved from 4.55591 to 4.55342, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5534 - val_loss: 4.5785\n",
      "Epoch 35/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5489Epoch 00034: loss improved from 4.55342 to 4.54927, saving model to weights.best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254016/254016 [==============================] - 16s - loss: 4.5493 - val_loss: 4.5760\n",
      "Epoch 36/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5460Epoch 00035: loss improved from 4.54927 to 4.54615, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5461 - val_loss: 4.5724\n",
      "Epoch 37/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5409Epoch 00036: loss improved from 4.54615 to 4.54089, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.5409 - val_loss: 4.5831\n",
      "Epoch 38/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5380Epoch 00037: loss improved from 4.54089 to 4.53824, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5382 - val_loss: 4.5756\n",
      "Epoch 39/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5328Epoch 00038: loss improved from 4.53824 to 4.53295, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5329 - val_loss: 4.5661\n",
      "Epoch 40/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5316Epoch 00039: loss improved from 4.53295 to 4.53192, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.5319 - val_loss: 4.5868\n",
      "Epoch 41/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5283Epoch 00040: loss improved from 4.53192 to 4.52821, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5282 - val_loss: 4.5632\n",
      "Epoch 42/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5246Epoch 00041: loss improved from 4.52821 to 4.52426, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.5243 - val_loss: 4.5524\n",
      "Epoch 43/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5203Epoch 00042: loss improved from 4.52426 to 4.52051, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.5205 - val_loss: 4.5572\n",
      "Epoch 44/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5182Epoch 00043: loss improved from 4.52051 to 4.51797, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.5180 - val_loss: 4.5453\n",
      "Epoch 45/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5154Epoch 00044: loss improved from 4.51797 to 4.51543, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5154 - val_loss: 4.5481\n",
      "Epoch 46/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5128Epoch 00045: loss improved from 4.51543 to 4.51312, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5131 - val_loss: 4.5393\n",
      "Epoch 47/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5103Epoch 00046: loss improved from 4.51312 to 4.51016, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5102 - val_loss: 4.5415\n",
      "Epoch 48/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5059Epoch 00047: loss improved from 4.51016 to 4.50619, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5062 - val_loss: 4.5472\n",
      "Epoch 49/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5037Epoch 00048: loss improved from 4.50619 to 4.50398, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5040 - val_loss: 4.5335\n",
      "Epoch 50/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.5023Epoch 00049: loss improved from 4.50398 to 4.50260, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.5026 - val_loss: 4.5277\n",
      "Epoch 51/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4994Epoch 00050: loss improved from 4.50260 to 4.49936, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4994 - val_loss: 4.5332\n",
      "Epoch 52/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4952Epoch 00051: loss improved from 4.49936 to 4.49545, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4955 - val_loss: 4.5284\n",
      "Epoch 53/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4937Epoch 00052: loss improved from 4.49545 to 4.49373, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4937 - val_loss: 4.5240\n",
      "Epoch 54/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4899Epoch 00053: loss improved from 4.49373 to 4.48970, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4897 - val_loss: 4.5312\n",
      "Epoch 55/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4901Epoch 00054: loss improved from 4.48970 to 4.48950, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4895 - val_loss: 4.5165\n",
      "Epoch 56/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4855Epoch 00055: loss improved from 4.48950 to 4.48567, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4857 - val_loss: 4.5615\n",
      "Epoch 57/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4819Epoch 00056: loss improved from 4.48567 to 4.48226, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4823 - val_loss: 4.5191\n",
      "Epoch 58/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4794Epoch 00057: loss improved from 4.48226 to 4.47943, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4794 - val_loss: 4.5095\n",
      "Epoch 59/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4776Epoch 00058: loss improved from 4.47943 to 4.47756, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4776 - val_loss: 4.5078\n",
      "Epoch 60/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4738Epoch 00059: loss improved from 4.47756 to 4.47377, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4738 - val_loss: 4.5316\n",
      "Epoch 61/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4714Epoch 00060: loss improved from 4.47377 to 4.47145, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4715 - val_loss: 4.5076\n",
      "Epoch 62/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4704Epoch 00061: loss improved from 4.47145 to 4.47063, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4706 - val_loss: 4.5207\n",
      "Epoch 63/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4681Epoch 00062: loss improved from 4.47063 to 4.46826, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4683 - val_loss: 4.4981\n",
      "Epoch 64/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4651Epoch 00063: loss improved from 4.46826 to 4.46499, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4650 - val_loss: 4.4958\n",
      "Epoch 65/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4632Epoch 00064: loss improved from 4.46499 to 4.46316, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4632 - val_loss: 4.4928\n",
      "Epoch 66/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4612Epoch 00065: loss improved from 4.46316 to 4.46137, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4614 - val_loss: 4.4917\n",
      "Epoch 67/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4598Epoch 00066: loss improved from 4.46137 to 4.45969, saving model to weights.best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254016/254016 [==============================] - 15s - loss: 4.4597 - val_loss: 4.4880\n",
      "Epoch 68/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4571Epoch 00067: loss improved from 4.45969 to 4.45689, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4569 - val_loss: 4.4880\n",
      "Epoch 69/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4583Epoch 00068: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4586 - val_loss: 4.4834\n",
      "Epoch 70/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4533Epoch 00069: loss improved from 4.45689 to 4.45356, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4536 - val_loss: 4.4934\n",
      "Epoch 71/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4522Epoch 00070: loss improved from 4.45356 to 4.45226, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4523 - val_loss: 4.4807\n",
      "Epoch 72/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4507Epoch 00071: loss improved from 4.45226 to 4.45031, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4503 - val_loss: 4.5620\n",
      "Epoch 73/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4512Epoch 00072: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4513 - val_loss: 4.5185\n",
      "Epoch 74/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4491Epoch 00073: loss improved from 4.45031 to 4.44923, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4492 - val_loss: 4.4854\n",
      "Epoch 75/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4472Epoch 00074: loss improved from 4.44923 to 4.44717, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4472 - val_loss: 4.5016\n",
      "Epoch 76/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4479Epoch 00075: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4480 - val_loss: 4.4991\n",
      "Epoch 77/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4465Epoch 00076: loss improved from 4.44717 to 4.44609, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4461 - val_loss: 4.4832\n",
      "Epoch 78/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4448Epoch 00077: loss improved from 4.44609 to 4.44462, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4446 - val_loss: 4.4861\n",
      "Epoch 79/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4438Epoch 00078: loss improved from 4.44462 to 4.44399, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4440 - val_loss: 4.4800\n",
      "Epoch 80/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4430Epoch 00079: loss improved from 4.44399 to 4.44310, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4431 - val_loss: 4.4773\n",
      "Epoch 81/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4431Epoch 00080: loss improved from 4.44310 to 4.44301, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4430 - val_loss: 4.4713\n",
      "Epoch 82/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4414Epoch 00081: loss improved from 4.44301 to 4.44144, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4414 - val_loss: 4.4868\n",
      "Epoch 83/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4406Epoch 00082: loss improved from 4.44144 to 4.44046, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4405 - val_loss: 4.4709\n",
      "Epoch 84/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4376Epoch 00083: loss improved from 4.44046 to 4.43763, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4376 - val_loss: 4.4767\n",
      "Epoch 85/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4391Epoch 00084: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4394 - val_loss: 4.4755\n",
      "Epoch 86/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4374Epoch 00085: loss improved from 4.43763 to 4.43745, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4375 - val_loss: 4.4738\n",
      "Epoch 87/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4373Epoch 00086: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4375 - val_loss: 4.4945\n",
      "Epoch 88/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4348Epoch 00087: loss improved from 4.43745 to 4.43486, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4349 - val_loss: 4.4652\n",
      "Epoch 89/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4337Epoch 00088: loss improved from 4.43486 to 4.43363, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4336 - val_loss: 4.4889\n",
      "Epoch 90/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4356Epoch 00089: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4353 - val_loss: 4.4793\n",
      "Epoch 91/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4316Epoch 00090: loss improved from 4.43363 to 4.43203, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4320 - val_loss: 4.4733\n",
      "Epoch 92/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4329Epoch 00091: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4330 - val_loss: 4.4640\n",
      "Epoch 93/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4298Epoch 00092: loss improved from 4.43203 to 4.42976, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4298 - val_loss: 4.4599\n",
      "Epoch 94/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4323Epoch 00093: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4320 - val_loss: 4.4634\n",
      "Epoch 95/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4294Epoch 00094: loss improved from 4.42976 to 4.42970, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4297 - val_loss: 4.4614\n",
      "Epoch 96/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4282Epoch 00095: loss improved from 4.42970 to 4.42852, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4285 - val_loss: 4.4585\n",
      "Epoch 97/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4277Epoch 00096: loss improved from 4.42852 to 4.42797, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4280 - val_loss: 4.4710\n",
      "Epoch 98/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4302Epoch 00097: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4300 - val_loss: 4.5099\n",
      "Epoch 99/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4272Epoch 00098: loss improved from 4.42797 to 4.42735, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4274 - val_loss: 4.4573\n",
      "Epoch 100/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4268Epoch 00099: loss improved from 4.42735 to 4.42713, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4271 - val_loss: 4.4581\n",
      "Epoch 101/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4255Epoch 00100: loss improved from 4.42713 to 4.42548, saving model to weights.best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254016/254016 [==============================] - 15s - loss: 4.4255 - val_loss: 4.4720\n",
      "Epoch 102/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4252Epoch 00101: loss improved from 4.42548 to 4.42537, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4254 - val_loss: 4.4720\n",
      "Epoch 103/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4255Epoch 00102: loss improved from 4.42537 to 4.42526, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4253 - val_loss: 4.4806\n",
      "Epoch 104/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4234Epoch 00103: loss improved from 4.42526 to 4.42433, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4243 - val_loss: 4.4861\n",
      "Epoch 105/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4233Epoch 00104: loss improved from 4.42433 to 4.42336, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4234 - val_loss: 4.4550\n",
      "Epoch 106/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4245Epoch 00105: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4242 - val_loss: 4.4611\n",
      "Epoch 107/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4219Epoch 00106: loss improved from 4.42336 to 4.42226, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4223 - val_loss: 4.4535\n",
      "Epoch 108/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4222Epoch 00107: loss improved from 4.42226 to 4.42198, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4220 - val_loss: 4.4527\n",
      "Epoch 109/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4235Epoch 00108: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4232 - val_loss: 4.4555\n",
      "Epoch 110/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4206Epoch 00109: loss improved from 4.42198 to 4.42065, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4206 - val_loss: 4.4547\n",
      "Epoch 111/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4193Epoch 00110: loss improved from 4.42065 to 4.41968, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4197 - val_loss: 4.4555\n",
      "Epoch 112/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4204Epoch 00111: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4209 - val_loss: 4.4528\n",
      "Epoch 113/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4187Epoch 00112: loss improved from 4.41968 to 4.41869, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4187 - val_loss: 4.4736\n",
      "Epoch 114/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4205Epoch 00113: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4204 - val_loss: 4.4564\n",
      "Epoch 115/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4196Epoch 00114: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4194 - val_loss: 4.4575\n",
      "Epoch 116/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4182Epoch 00115: loss improved from 4.41869 to 4.41790, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4179 - val_loss: 4.4644\n",
      "Epoch 117/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4178Epoch 00116: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4184 - val_loss: 4.4847\n",
      "Epoch 118/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4175Epoch 00117: loss improved from 4.41790 to 4.41745, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4175 - val_loss: 4.4484\n",
      "Epoch 119/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4177Epoch 00118: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4180 - val_loss: 4.4650\n",
      "Epoch 120/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4167Epoch 00119: loss improved from 4.41745 to 4.41659, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4166 - val_loss: 4.4535\n",
      "Epoch 121/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4168Epoch 00120: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4166 - val_loss: 4.4458\n",
      "Epoch 122/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4156Epoch 00121: loss improved from 4.41659 to 4.41565, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4157 - val_loss: 4.4471\n",
      "Epoch 123/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4165Epoch 00122: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4164 - val_loss: 4.4487\n",
      "Epoch 124/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4156Epoch 00123: loss improved from 4.41565 to 4.41524, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4152 - val_loss: 4.4487\n",
      "Epoch 125/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4135Epoch 00124: loss improved from 4.41524 to 4.41358, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4136 - val_loss: 4.5244\n",
      "Epoch 126/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4166Epoch 00125: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4167 - val_loss: 4.4490\n",
      "Epoch 127/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4142Epoch 00126: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4146 - val_loss: 4.4445\n",
      "Epoch 128/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4145Epoch 00127: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4144 - val_loss: 4.4503\n",
      "Epoch 129/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4151Epoch 00128: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4148 - val_loss: 4.4568\n",
      "Epoch 130/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4124Epoch 00129: loss improved from 4.41358 to 4.41260, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4126 - val_loss: 4.4548\n",
      "Epoch 131/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4120Epoch 00130: loss improved from 4.41260 to 4.41205, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4121 - val_loss: 4.4476\n",
      "Epoch 132/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4130Epoch 00131: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4131 - val_loss: 4.4562\n",
      "Epoch 133/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4119Epoch 00132: loss improved from 4.41205 to 4.41200, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4120 - val_loss: 4.4439\n",
      "Epoch 134/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4113Epoch 00133: loss improved from 4.41200 to 4.41147, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4115 - val_loss: 4.4414\n",
      "Epoch 135/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4123Epoch 00134: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4122 - val_loss: 4.4433\n",
      "Epoch 136/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4107Epoch 00135: loss improved from 4.41147 to 4.41115, saving model to weights.best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254016/254016 [==============================] - 16s - loss: 4.4111 - val_loss: 4.4785\n",
      "Epoch 137/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4118Epoch 00136: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4120 - val_loss: 4.4418\n",
      "Epoch 138/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4113Epoch 00137: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4116 - val_loss: 4.4621\n",
      "Epoch 139/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4095Epoch 00138: loss improved from 4.41115 to 4.40927, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4093 - val_loss: 4.4607\n",
      "Epoch 140/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4108Epoch 00139: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4109 - val_loss: 4.4391\n",
      "Epoch 141/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4099Epoch 00140: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4101 - val_loss: 4.4424\n",
      "Epoch 142/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4111Epoch 00141: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4113 - val_loss: 4.4395\n",
      "Epoch 143/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4101Epoch 00142: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4095 - val_loss: 4.4438\n",
      "Epoch 144/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4096Epoch 00143: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4097 - val_loss: 4.4435\n",
      "Epoch 145/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4083Epoch 00144: loss improved from 4.40927 to 4.40825, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4083 - val_loss: 4.4382\n",
      "Epoch 146/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4086Epoch 00145: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4085 - val_loss: 4.4465\n",
      "Epoch 147/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4089Epoch 00146: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4088 - val_loss: 4.4471\n",
      "Epoch 148/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4088Epoch 00147: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4083 - val_loss: 4.4372\n",
      "Epoch 149/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4066Epoch 00148: loss improved from 4.40825 to 4.40665, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4066 - val_loss: 4.4415\n",
      "Epoch 150/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4073Epoch 00149: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4077 - val_loss: 4.4444\n",
      "Epoch 151/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4077Epoch 00150: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4078 - val_loss: 4.4516\n",
      "Epoch 152/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4084Epoch 00151: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4083 - val_loss: 4.4341\n",
      "Epoch 153/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4053Epoch 00152: loss improved from 4.40665 to 4.40500, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4050 - val_loss: 4.4382\n",
      "Epoch 154/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4067Epoch 00153: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4072 - val_loss: 4.4346\n",
      "Epoch 155/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4076Epoch 00154: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4073 - val_loss: 4.4494\n",
      "Epoch 156/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4050Epoch 00155: loss improved from 4.40500 to 4.40490, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4049 - val_loss: 4.4366\n",
      "Epoch 157/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4052Epoch 00156: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4050 - val_loss: 4.4364\n",
      "Epoch 158/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4057Epoch 00157: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4059 - val_loss: 4.4384\n",
      "Epoch 159/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4041Epoch 00158: loss improved from 4.40490 to 4.40450, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4045 - val_loss: 4.4506\n",
      "Epoch 160/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4050Epoch 00159: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4048 - val_loss: 4.4408\n",
      "Epoch 161/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4081Epoch 00160: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4082 - val_loss: 4.4380\n",
      "Epoch 162/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4045Epoch 00161: loss improved from 4.40450 to 4.40431, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4043 - val_loss: 4.4560\n",
      "Epoch 163/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4031Epoch 00162: loss improved from 4.40431 to 4.40303, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4030 - val_loss: 4.4518\n",
      "Epoch 164/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4039Epoch 00163: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4045 - val_loss: 4.4401\n",
      "Epoch 165/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4052Epoch 00164: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4053 - val_loss: 4.4346\n",
      "Epoch 166/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4030Epoch 00165: loss improved from 4.40303 to 4.40284, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4028 - val_loss: 4.4338\n",
      "Epoch 167/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4062Epoch 00166: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4062 - val_loss: 4.4498\n",
      "Epoch 168/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4038Epoch 00167: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4041 - val_loss: 4.4309\n",
      "Epoch 169/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4024Epoch 00168: loss improved from 4.40284 to 4.40254, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4025 - val_loss: 4.4402\n",
      "Epoch 170/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4026Epoch 00169: loss improved from 4.40254 to 4.40244, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4024 - val_loss: 4.4316\n",
      "Epoch 171/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4017Epoch 00170: loss improved from 4.40244 to 4.40192, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4019 - val_loss: 4.4626\n",
      "Epoch 172/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4022Epoch 00171: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4024 - val_loss: 4.4348\n",
      "Epoch 173/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4024Epoch 00172: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254016/254016 [==============================] - 16s - loss: 4.4024 - val_loss: 4.4449\n",
      "Epoch 174/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4043Epoch 00173: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4039 - val_loss: 4.4457\n",
      "Epoch 175/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4005Epoch 00174: loss improved from 4.40192 to 4.40082, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.4008 - val_loss: 4.4321\n",
      "Epoch 176/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4029Epoch 00175: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4028 - val_loss: 4.4421\n",
      "Epoch 177/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4005Epoch 00176: loss improved from 4.40082 to 4.40027, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.4003 - val_loss: 4.4334\n",
      "Epoch 178/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4012Epoch 00177: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4016 - val_loss: 4.4330\n",
      "Epoch 179/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4027Epoch 00178: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4028 - val_loss: 4.4328\n",
      "Epoch 180/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4002Epoch 00179: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4005 - val_loss: 4.4377\n",
      "Epoch 181/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4014Epoch 00180: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4012 - val_loss: 4.4351\n",
      "Epoch 182/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4008Epoch 00181: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4006 - val_loss: 4.4341\n",
      "Epoch 183/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3992Epoch 00182: loss improved from 4.40027 to 4.39955, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.3995 - val_loss: 4.4754\n",
      "Epoch 184/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4027Epoch 00183: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.4024 - val_loss: 4.4314\n",
      "Epoch 185/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3985Epoch 00184: loss improved from 4.39955 to 4.39860, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.3986 - val_loss: 4.4331\n",
      "Epoch 186/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4014Epoch 00185: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4013 - val_loss: 4.4355\n",
      "Epoch 187/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4002Epoch 00186: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4005 - val_loss: 4.4358\n",
      "Epoch 188/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3979Epoch 00187: loss improved from 4.39860 to 4.39813, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.3981 - val_loss: 4.4378\n",
      "Epoch 189/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.4014Epoch 00188: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.4013 - val_loss: 4.4312\n",
      "Epoch 190/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3991Epoch 00189: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.3993 - val_loss: 4.4354\n",
      "Epoch 191/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3997Epoch 00190: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.3994 - val_loss: 4.4323\n",
      "Epoch 192/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3984Epoch 00191: loss improved from 4.39813 to 4.39812, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.3981 - val_loss: 4.4268\n",
      "Epoch 193/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3986Epoch 00192: loss improved from 4.39812 to 4.39806, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.3981 - val_loss: 4.4316\n",
      "Epoch 194/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3998Epoch 00193: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.3992 - val_loss: 4.4409\n",
      "Epoch 195/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3983Epoch 00194: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.3985 - val_loss: 4.4543\n",
      "Epoch 196/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3980Epoch 00195: loss improved from 4.39806 to 4.39793, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 16s - loss: 4.3979 - val_loss: 4.4300\n",
      "Epoch 197/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3975Epoch 00196: loss improved from 4.39793 to 4.39733, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.3973 - val_loss: 4.4329\n",
      "Epoch 198/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3975Epoch 00197: loss did not improve\n",
      "254016/254016 [==============================] - 15s - loss: 4.3977 - val_loss: 4.4302\n",
      "Epoch 199/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3988Epoch 00198: loss did not improve\n",
      "254016/254016 [==============================] - 16s - loss: 4.3988 - val_loss: 4.4323\n",
      "Epoch 200/200\n",
      "253440/254016 [============================>.] - ETA: 0s - loss: 4.3969Epoch 00199: loss improved from 4.39733 to 4.39690, saving model to weights.best.h5\n",
      "254016/254016 [==============================] - 15s - loss: 4.3969 - val_loss: 4.4324\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='loss', patience=10, factor=0.1, min_lr=0.00001)\n",
    "filepath=\"weights.best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='loss', patience=20, verbose=1), reduce_lr, checkpoint]\n",
    "\n",
    "# callbacks = [EarlyStopping(monitor='loss', patience=20, verbose=1), reduce_lr]\n",
    "\n",
    "results = model.fit(X_train, Y_train, batch_size=512, epochs=200, validation_split = 0.2, shuffle = True, callbacks = callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b5f2606d470>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b59eba9a438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = results.history['loss']\n",
    "val_loss = results.history['val_loss'] \n",
    "# train_acc = results.history['acc']\n",
    "# val_acc = results.history['val_acc']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model.evaluate(X_test, Y_test, verbose=0) # accuracy check\n",
    "# print(score.shape)\n",
    "# print('Test loss:', score[0]) # Prints test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Serialize weights to H5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Load model and weights\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load weights into new model\n",
    "loaded_model.load_weights(\"weights.best.h5\")\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAGGpJREFUeJztnV1sXVV2x/8rzrftxHYMieNkGhIilShiEmSFoIERndEgigZBJITgAfGAJqNqkIo0fUBUKlTqA1MVEA+IKpRomIry0eErqlA7FI2E5oWJoZCEJIVAHE2cDydxvhMSf6w+3BPV8Zz1v/ce2+cms/8/yfL1Xnefve4+Z/ncu/93rW3uDiFEekxrtANCiMag4BciURT8QiSKgl+IRFHwC5EoCn4hEkXBL0SiKPiFSBQFvxCJMn0inc3sTgDPA2gC8C/u/jR7fkdHh3d3d+fapk2L/w9FNjML+4yOjoa2kZGR0DY8PFx3v5aWlrAP+wYl8+NKgfk4fXp8+UQ2dryLFy+GNnZe2HXQ1NSU2z537tywD7t2hoaGQhuDzVV0fTM/ovno7+/H4OBgPCFjfarlSXmYWROAFwD8CMB+AFvNbIu774z6dHd3Y8uWLbm2WbNmhWM1NzfntkcnFuAX0uDgYGg7evRoaDt16lRu+2233Rb2uXDhQmg7c+ZMaGMXO4PNSRHYXC1cuDC0tbe357az17xv377Qxs7LzJkzQ9v8+fNz29euXRv2YT4eOnQotLF/Qp2dnaEtur6ZH9F8bNiwIewznom87V8HYI+7f+PuFwG8DuCeCRxPCFEiEwn+bgB/GPP3/qxNCHEVMOULfma20cx6zayXvYUUQpTLRIK/H8DSMX8vydouw903uXuPu/d0dHRMYDghxGQykeDfCmClmV1nZjMBPAAgfzVPCHHFUXi1392HzexRAP+FitS32d2/mDTPLh+rrvZqNkYR2YitADMVY86cOaGNSWJMQYj6MampiNICcEUiWqlmvs+ePTu0zZs3r5Af0XinT58O+3z77behjfnIVAcmZReRDyOpko0zngnp/O7+PoD3J3IMIURj0Df8hEgUBb8QiaLgFyJRFPxCJIqCX4hEmdBq/2RSJAuvaOYeY8aMGaEtknK++uqrsM/SpUtD25IlS0Ibk+aYtBUlNDEZikmOTDJlktjx48dz21niUWtra2hjcuTJkyfrtu3evTvsw76MxhJ0mAzI5Lzz58/ntjPZLpI+60ns0p1fiERR8AuRKAp+IRJFwS9Eoij4hUiUUlf7zSxMnGGrytFKKVsNZWW8WPIOW+2PVsW//vrrsE9URqoabHWe+Vhk5ZiNxc7L2bNn67YxZYHNFVMCipRs6+vrC/swpWXx4sWhjSkSbLU/Um/YeYkSrupJ7NGdX4hEUfALkSgKfiESRcEvRKIo+IVIFAW/EIlyxST2FJH6ih6PweSVKGmCJRhF0hvA68gxObKIbMf6sEQh9tqKwPxgSSnMxs5ZJL+xpCQmHTIbS+xh12M0x+waYLZa0Z1fiERR8AuRKAp+IRJFwS9Eoij4hUgUBb8QiTIhqc/M+gCcBjACYNjde4oei0lKRWS7ohIVk40irr322tDGfD98+HBoY5ISy3CLbEwuPXXqVGhjMiCTm1paWnLbWeYbq7vIbCzLsb29Pbed1f1jRLUJgampGxkRScj1XPeTofP/hbsfnYTjCCFKRG/7hUiUiQa/A/iNmX1iZhsnwyEhRDlM9G3/re7eb2bXAvjAzHa7+0djn5D9U9gI8CooQohymdCd3937s98DAN4BsC7nOZvcvcfdexYsWDCR4YQQk0jh4DezZjNrvfQYwB0AdkyWY0KIqWUib/sXAngnk3umA/g3d//PogdjkliR7bqmInssOuaKFSvCPkePxkJIf39/aLtw4UJomzt3bmiLpD6WQch8ZOeFSY5tbW2hLYJl2jEbO2fR9lrsdbG5Z3PFMjjZu97ofDLp8MyZM7ntpUh97v4NgO8W7S+EaCyS+oRIFAW/EImi4BciURT8QiSKgl+IRLliCngyIvmiSFFEoJicB8R7uHV1dYV9WBbYiRMnQhvzkb22KCuRZeBFslE15s2bF9qirD4mX7HsQiajRWMB3McIlm0Z7f0H8NfG9iGMsvrY8SYjq093fiESRcEvRKIo+IVIFAW/EImi4BciUUpd7R8aGsKhQ4dybUW2YypS+wzgK8esdl60+rp69eqwD6thwJSFs2fPhrYvv/wytB05ciS3nSXaLF++PLSdO3cutDEGBgYK9Ytg1wdTTaKVe1Z3cc6cOaGNzSO7dorUa4ySkgDgO9/5Tm57PTUodecXIlEU/EIkioJfiERR8AuRKAp+IRJFwS9EopQq9U2bNg3Nzc1192NJKUVgstHs2bPrPl4krwF8eyomAx48eDC0RXIpECfHFKlzB3BZNEp0AriMWQQm6xbZJotJYux47DWzfiwJLZL62NxH86vEHiFEVRT8QiSKgl+IRFHwC5EoCn4hEkXBL0SiVJX6zGwzgB8DGHD31VlbB4A3ACwD0AfgfnePi9VlNDU1hVs8MZlkeHg4t31oaKjakLkw2YhJOZFcxrZwYnIeq+vGav8VqXXHstGWLVsW2phkV2RLNCbbMjmyKNE1wqQ+Jpexay66ToFi24OdPHky7BNtX8Z8GE8td/5fArhzXNvjAD5095UAPsz+FkJcRVQNfnf/CMD4kqX3AHgle/wKgHsn2S8hxBRT9DP/Qne/9BW0Q6js2CuEuIqY8IKfVz7MhB9ozGyjmfWaWe+xY8cmOpwQYpIoGvyHzawLALLfYc0md9/k7j3u3sP2KBdClEvR4N8C4OHs8cMA3pscd4QQZVGL1PcagNsBdJrZfgBPAngawJtm9giAfQDur2WwadOmYe7cubm2SO4AYpmkngymsRQpFgrEGXpMOmRbYZ0+fTq0MTmP+RhJaZE0BAB9fX2hjW13VWRLtKLZfmyOmR/RfBTNEmT9mI3Nf3TtM1kxmt96MhyrBr+7PxiYfljzKEKIKw59w0+IRFHwC5EoCn4hEkXBL0SiKPiFSJRSC3iOjo6GxQpZ1tNkZ3ux4xUptMjksP3794e2AwcOhDZGe3t7aIukVCY17dy5M7StXbs2tBWRWlkfJgOy88KIjslkNDYWy0osmgEZXVf1ZOhdoh6pT3d+IRJFwS9Eoij4hUgUBb8QiaLgFyJRFPxCJEqpUt/IyEhYmJIVzoyy2Fh2G4PJPEwSi6QXJh0ePnw4tO3bty+0Mfmwq6srtHV0dOS2s73/BgfHV2n7f1hWIstiizIgi2QkAlx+Y/JWZDt37lzYh8mRbCzmf5H9Jou85nokUd35hUgUBb8QiaLgFyJRFPxCJIqCX4hEKXW1f3h4GAMD+YV+2ep2tII9e/bsQn6w1X62uh3V1WN92JZLbLWcKQhspZrV/otgiUKsziDzP5pjds6Y4sNWsYuszhdJmpkI7LVFvky1j7rzC5EoCn4hEkXBL0SiKPiFSBQFvxCJouAXIlFq2a5rM4AfAxhw99VZ21MAfgLgSPa0J9z9/WrHGhkZCaUoJhtFMg+TTxhMNjp//nxoi3zfu3dv2GfOnDmhjUlsLAGGyYeR/83NzWGfRYsWhTa2jVqRenysll3R7deKJMAUrSXIJNiiiT1FtjaLbPUkENVy5/8lgDtz2p9z9zXZT9XAF0JcWVQNfnf/CECc8ymEuCqZyGf+R81sm5ltNrP4/asQ4oqkaPC/CGAFgDUADgJ4JnqimW00s14z6y3y1VMhxNRQKPjd/bC7j7j7KICXAKwjz93k7j3u3sO+vy+EKJdCwW9mY+tIbQCwY3LcEUKURS1S32sAbgfQaWb7ATwJ4HYzWwPAAfQB+Gktg42OjobS0bFjx8J+UV09thUWk3/Onj1byBbJaAsWLAj7sCw2JucxmYfJOdF2XW1tbWEfZmN1BplcFmX1FZXD2FwxmTg6JnsXWjRLk107TCKMJN/ly5eHfebPn5/b3tLSEvYZT9Xgd/cHc5pfrnkEIcQVib7hJ0SiKPiFSBQFvxCJouAXIlEU/EIkSqkFPKdPn47Ozs5cGytWePHixdx2loHHinRGx6vWL4L5wSRHJucVlQEj+W0qfGT9ihSfZHIYmw+W3RltG8ZkOSZhRlIqGwvg23xF1xzbYi3aBo5tNzce3fmFSBQFvxCJouAXIlEU/EIkioJfiERR8AuRKKVKfTNnzkR3d3eujRX6iGSNo0ePhn1YZhaDSUqRjclojKIZf6woaCTNMXmTyV6s8GcR+Yr1YbJiUYkt6ld0D8Ki+wmya+T48eN1tQPxfo2S+oQQVVHwC5EoCn4hEkXBL0SiKPiFSJRSV/tnzJiBrq6u6k8cx5EjR3Lb2Sr1iRMn6h4HAFpbW+vuM9kJLgBf0Wc1A6OadUxNYUpAkW3UgPh1s9V+NlesHyNSEJiKwfxg25dFK/AAvx6jVX12ziL1oJ7rTXd+IRJFwS9Eoij4hUgUBb8QiaLgFyJRFPxCJEot23UtBfArAAtR2Z5rk7s/b2YdAN4AsAyVLbvud/c4E6FyrDBhhUlbkSzD5BomAzJpiyVGRLXdWEIKk6jqScIYC5Mjo0QWlijEKFpXL5IcmTzIkmbYPDKJLTpn7HpjCTWR7AzECWgAl/oieY7NR+Q/uxb/6Lk1PGcYwM/dfRWA9QB+ZmarADwO4EN3Xwngw+xvIcRVQtXgd/eD7v5p9vg0gF0AugHcA+CV7GmvALh3qpwUQkw+dX3mN7NlANYC+BjAQne/VFv4ECofC4QQVwk1B7+ZtQB4C8Bj7n7Z9w698kEu98OcmW00s14z62XFN4QQ5VJT8JvZDFQC/1V3fztrPmxmXZm9C8BAXl933+TuPe7eE23YIYQon6rBb5Vl25cB7HL3Z8eYtgB4OHv8MID3Jt89IcRUUUtW3/cAPARgu5l9lrU9AeBpAG+a2SMA9gG4v9qBRkdHw2wkJlG0tbXltkf1AAEuQw0ODoY2JhFGdQGZJFNUVpw3b15oW7JkSWhjte4imIzGas8VkaKKwrLV2PmM6OjoKHQ8JucxG5vHSE6NrnsglrnrkfqqBr+7/w5AvmgL/LDmkYQQVxT6hp8QiaLgFyJRFPxCJIqCX4hEUfALkSilFvAcHh4Ot9hislF7e3tuO8vqY1IZK0q5d+/e0BYVVIwyxwDg5MmToS3KfKtmW7x4cWiLJCw2v2zbMCZHsn6RjWUJMlmUZcUxW7Qt14EDBwr5wTII2Tlj0ic7NxGRjyxrcjy68wuRKAp+IRJFwS9Eoij4hUgUBb8QiaLgFyJRSpX6RkZGQrls/vz5Yb8ou4lJJEzOY/Iby4qK5BW2pxorYMKy6ViRTnbMKOOPSU1MomLzweY4Go9lW7KMSiZhRXIeEGfosWuA+djS0hLamLzMJM7oOmCSYzRX9expqDu/EImi4BciURT8QiSKgl+IRFHwC5Eopa72NzU1hck4bAU7qo22cGG8VQCzrV69OrSxFedoG6cXXngh7LNmzZrQdscdd4S29evXh7ZFixaFtmjl+8KFC2EftmrP6gUytaXI9mBMWWCr/Wysrq6u3PZ333037MMSp5gqxeaRXVdRfT/2uiJloZ4kId35hUgUBb8QiaLgFyJRFPxCJIqCX4hEUfALkShVdQEzWwrgV6hswe0ANrn782b2FICfADiSPfUJd3+fHWvatGmhRMHq4EUJDkwKKSKTAFzmWblyZW57T09P2IfJcoyiSS5R7TwmQzGK1JcD4nPGEk9YIgvrx15bdF3dfPPNYR8290ySZn6whKCo7iLboiySdFkcjaeWMzsM4Ofu/qmZtQL4xMw+yGzPufs/1TyaEOKKoZa9+g4COJg9Pm1muwDEO2QKIa4K6vrMb2bLAKwF8HHW9KiZbTOzzWaWX19bCHFFUnPwm1kLgLcAPObupwC8CGAFgDWovDN4Jui30cx6zay3yFbKQoipoabgN7MZqAT+q+7+NgC4+2F3H3H3UQAvAViX19fdN7l7j7v3sD3RhRDlUjX4rVLj6WUAu9z92THtYzMmNgDYMfnuCSGmilpW+78H4CEA283ss6ztCQAPmtkaVOS/PgA/rTrY9OlhVhSTKCLphdVFYzYmuyxbtiy0RfLhfffdF/Y5c+ZMaGPyFdsWivWLXhuTodiWXKzf0NBQaIuyCNnrYtIWe81FsgtvueWWsM+OHfF9bPv27aGNzSPLMu3s7MxtZzERZQJOqtTn7r8DkFfhkWr6QogrG33DT4hEUfALkSgKfiESRcEvRKIo+IVIlFILeBYlymJjWWCRFAJwqa+9Pf6W8qxZs3Lb77777rDP7t27Q9uePXtCG5Ns2PZaUVYfk8OYZMfGKiJVMqmP+cEkx+i8ALHUx66BBQsWhDZ2ffT19YU2dq6PHDmS286KhUbbobHzNR7d+YVIFAW/EImi4BciURT8QiSKgl+IRFHwC5EopUp9w8PDOHnyZK6NZURFEhCTw6JxAF4Ak0likaS0atWqsA+Tr5jsxfyI5DygPqnnEmw+2B5/7JxFMiybD3Y+mY3t8RfNx4EDB8I+ra2toe36668Pbeya27p1a2iL5viGG24I+9x444257Wwu/ui5NT9TCPEnhYJfiERR8AuRKAp+IRJFwS9Eoij4hUiUUqW+0dHRsBgnk3KiYpxM1mAyFIPJaEX2rWMZYsuXLw9tbD6am5tDG8t0LNKnaCHRSD4ssq8ewAuyMj8iBgYGQtuSJUtCGyvwGmXnAbzw57Fjx3LbWfFXJs/Wiu78QiSKgl+IRFHwC5EoCn4hEkXBL0SiVF2+NrPZAD4CMCt7/q/d/Ukzuw7A6wAWAPgEwEPuXnWpOVqZjWqtAfHqdtH6cszGVIJotT9arQX462Irx0WJVufZSjqbj1OnToU2tgIfreqzensMtrrNkoWi8zl37tywD1N1WL+lS5eGtjVr1oS2efPm1dUOxNcc2/JsPLXc+S8A+IG7fxeV7bjvNLP1AH4B4Dl3vx7AcQCP1DyqEKLhVA1+r3BJcJyR/TiAHwD4ddb+CoB7p8RDIcSUUNNnfjNrynboHQDwAYCvAZxw90vvMfYD6J4aF4UQU0FNwe/uI+6+BsASAOsA/HmtA5jZRjPrNbPewcHBgm4KISabulb73f0EgN8CuAVAm5ldWhlZAqA/6LPJ3Xvcvaejo2NCzgohJo+qwW9m15hZW/Z4DoAfAdiFyj+B+7KnPQzgvalyUggx+dSSqdIF4BUza0Lln8Wb7v4fZrYTwOtm9g8A/gfAy7UMGMlDLKEm2lqJSX1Fa88xIrmMJYmw7Z06OztDG0taYvJbVEeOJe+weYwSsQB+ziKJk0mfzI8ich4Qv+5rrrkm7MN8ZFIaO9dRzT0AaGtry21niULRR+h6pL6qwe/u2wCszWn/BpXP/0KIqxB9w0+IRFHwC5EoCn4hEkXBL0SiKPiFSBSbjFpgNQ9mdgTAvuzPTgBHSxs8Rn5cjvy4nKvNjz9z91jHHEOpwX/ZwGa97t7TkMHlh/yQH3rbL0SqKPiFSJRGBv+mBo49FvlxOfLjcv5k/WjYZ34hRGPR234hEqUhwW9md5rZ/5rZHjN7vBE+ZH70mdl2M/vMzHpLHHezmQ2Y2Y4xbR1m9oGZfZX9jlPEptaPp8ysP5uTz8zsrhL8WGpmvzWznWb2hZn9ddZe6pwQP0qdEzObbWa/N7PPMz/+Pmu/zsw+zuLmDTOL0w9rwd1L/QHQhEoZsOUAZgL4HMCqsv3IfOkD0NmAcb8P4CYAO8a0/SOAx7PHjwP4RYP8eArA35Q8H10AbsoetwL4EsCqsueE+FHqnAAwAC3Z4xkAPgawHsCbAB7I2v8ZwF9NZJxG3PnXAdjj7t94pdT36wDuaYAfDcPdPwIwPiH7HlQKoQIlFUQN/Cgddz/o7p9mj0+jUiymGyXPCfGjVLzClBfNbUTwdwP4w5i/G1n80wH8xsw+MbONDfLhEgvd/WD2+BCAhQ305VEz25Z9LJjyjx9jMbNlqNSP+BgNnJNxfgAlz0kZRXNTX/C71d1vAvCXAH5mZt9vtENA5T8/Kv+YGsGLAFagskfDQQDPlDWwmbUAeAvAY+5+WbmiMuckx4/S58QnUDS3VhoR/P0Axm5tEhb/nGrcvT/7PQDgHTS2MtFhM+sCgOx3XBtsCnH3w9mFNwrgJZQ0J2Y2A5WAe9Xd386aS5+TPD8aNSfZ2HUXza2VRgT/VgArs5XLmQAeALClbCfMrNnMWi89BnAHgB2815SyBZVCqEADC6JeCraMDShhTqxSvO9lALvc/dkxplLnJPKj7DkprWhuWSuY41Yz70JlJfVrAH/bIB+Wo6I0fA7gizL9APAaKm8fh1D57PYIKnsefgjgKwD/DaCjQX78K4DtALahEnxdJfhxKypv6bcB+Cz7uavsOSF+lDonAG5EpSjuNlT+0fzdmGv29wD2APh3ALMmMo6+4SdEoqS+4CdEsij4hUgUBb8QiaLgFyJRFPxCJIqCX4hEUfALkSgKfiES5f8ApyFEgpiFhwMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b5f2642cef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b5f2642c780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b5f26527320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 25\n",
    "test_img = X_test[k].reshape(1, 32, 32, 1)\n",
    "pred_img = model.predict(test_img)\n",
    "pred_img = pred_img.reshape(32,32)\n",
    "pred_img = pred_img.astype('int') \n",
    "\n",
    "plt.imshow(X_test[k].reshape(32,32), cmap='gray')\n",
    "plt.figure()\n",
    "plt.imshow(pred_img, cmap='gray')\n",
    "plt.figure()\n",
    "plt.imshow(Y_test[k].reshape(32,32), cmap='gray')\n",
    "\n",
    "Y_test_img = Y_test[k].reshape(32,32)\n",
    "X_test_img = X_test[k].reshape(32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Y, X):  66.3505859375\n",
      "MSE (Y , Predict):  41.9853515625\n",
      "\n",
      "MAE (Y, X):  6.2470703125\n",
      "MAE (Y , Predict):  4.5263671875\n",
      "\n",
      "SSIM (Y, X):  1.0\n",
      "SSIM (Y, Predict):  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg/python/3.6.2/install/lib/python3.6/site-packages/skimage/util/arraycrop.py:177: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  cropped = ar[slices]\n"
     ]
    }
   ],
   "source": [
    "# Compute MSE\n",
    "print('MSE (Y, X): ', mean_squared_error(Y_test_img, X_test_img))\n",
    "print('MSE (Y , Predict): ', mean_squared_error(Y_test_img, pred_img))\n",
    "\n",
    "# Compute MAE\n",
    "print('\\nMAE (Y, X): ', mean_absolute_error(Y_test_img, X_test_img))\n",
    "print('MAE (Y , Predict): ', mean_absolute_error(Y_test_img, pred_img))\n",
    "\n",
    "# Compute SSIM\n",
    "(score1, diff1) = compare_ssim(Y_test_img, X_test_img, full=True)\n",
    "print('\\nSSIM (Y, X): ', score1)\n",
    "(score2, diff2) = compare_ssim(Y_test_img, pred_img, full=True)\n",
    "print('SSIM (Y, Predict): ', score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PSNR\n",
    "def psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2)**2)\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n",
    "\n",
    "psnr_X_test = psnr(Y_test_img, X_test_img)\n",
    "print('\\nPSNR (Y, X): ', psnr_X_test)\n",
    "psnr_predict = psnr(Y_test_img, pred_img)\n",
    "print('PSNR (Y, Pred): ', psnr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_X_img = abs(Y_test_img - X_test_img)\n",
    "plt.imshow(err_X_img)\n",
    "\n",
    "err_pred_img = abs(Y_test_img - pred_img)\n",
    "plt.figure()\n",
    "plt.imshow(err_pred_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
